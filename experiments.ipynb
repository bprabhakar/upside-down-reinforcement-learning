{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from model import *\n",
    "from utils import *\n",
    "from replay_buffer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this notebook uses Comet.ml for experiment tracking. If you don't have an account, please go here and create one for free - https://www.comet.ml/site/. After you create one, please input your details in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following code anywhere in your machine learning file\n",
    "comet_api_key = None\n",
    "comet_project_name = \"udrl\"\n",
    "comet_workspace = None\n",
    "experiment = Experiment(api_key=comet_api_key, project_name=comet_project_name, workspace=comet_workspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about Comet quickstart - https://www.comet.ml/docs/quick-start/#quick-start-for-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparams\n",
    "NUM_WARMUP_EPISODES   = 10    # No of warm-up episodes at the beginning\n",
    "REPLAY_SIZE           = 300   # Max size of the replay buffer (in episodes)\n",
    "RETURN_SCALE          = 0.01  # Scaling factor for desired horizon input (reward)\n",
    "HORIZON_SCALE         = 0.01  # Scaling factor for desired horizon input (steps)\n",
    "\n",
    "# training hyperparams\n",
    "BATCH_SIZE            = 512   # No of (input, target) pairs/batch for training \n",
    "                              # the behavior function\n",
    "NUM_UPDATES_PER_ITER  = 100   # No of gradient-based updates of the behavior \n",
    "                              # function per step of UDRL training\n",
    "LEARNING_RATE         = 1e-3  # LR for ADAM optimizer\n",
    "# generating episodes hyperparams\n",
    "\n",
    "NUM_EPISODES_PER_ITER = 10    # No of exploratory episodes generated per step of\n",
    "                              # UDRL training\n",
    "LAST_FEW              = 25    # No of episodes from the end of the replay buffer \n",
    "                              # used for sampling exploratory commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving `Sparse Lunar Lander`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Initialize replay buffer and warm-up using random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = 0; playing_step = 0\n",
    "\n",
    "replay_buffer = ReplayBuffer(REPLAY_SIZE)   # init replay buffer\n",
    "env = gym.make(\"LunarLander-v2\")            # init gym env\n",
    "\n",
    "for _ in tqdm(range(NUM_WARMUP_EPISODES)):\n",
    "\n",
    "    episode = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'next_states': []\n",
    "    }\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        episode['states'].append(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode['actions'].append(action)\n",
    "        episode['next_states'].append(state)\n",
    "        if not done: \n",
    "            episode['rewards'].append(0)         # because 'sparse' lunar lander\n",
    "    episode['rewards'].append(episode_reward)    # finally add total episode reward\n",
    "    \n",
    "    playing_step += 1\n",
    "    experiment.log_metric(\"episode_reward\", episode_reward, step=playing_step)\n",
    "\n",
    "    # add episode data to the replay buffer\n",
    "    replay_buffer.add_episode(\n",
    "        np.array(episode['states'], dtype=np.float),\n",
    "        np.array(episode['actions'], dtype=np.int),\n",
    "        np.array(episode['rewards'], dtype=np.float),\n",
    "        np.array(episode['next_states'], dtype=np.float),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Initialize policy network (BehaviorNet) and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy = BehaviorNet(state_dim, action_dim)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Main learning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - First, train policy network by sampling behavior segments from buffer.\n",
    "    \n",
    "    - Second, sample exploratory commands for future exploration.\n",
    "    \n",
    "    - Third, use the latest policy network & sampled commands to generate new trajectories & add them to the replay buffer\n",
    "    \n",
    "    - Continue looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while(1): # keep cycling indefinitely\n",
    "\n",
    "        # 1 - Train Policy Network\n",
    "        \n",
    "        episodes_to_train = replay_buffer.sample_episodes(5)\n",
    "        train_dset = BehaviorDataset(episodes_to_train, \n",
    "                                    size=BATCH_SIZE*NUM_UPDATES_PER_ITER, \n",
    "                                    horizon_scale=HORIZON_SCALE, \n",
    "                                    return_scale=RETURN_SCALE)\n",
    "        training_behaviors = TorchDataLoader(train_dset, \n",
    "                                            batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        if not policy.training: policy.train();\n",
    "        for behavior_batch in training_behaviors: # this runs for NUM_UPDATES_PER_ITER rounds\n",
    "            policy.zero_grad()\n",
    "            logprobs = policy(behavior_batch['features'])\n",
    "            loss = loss_func(logprobs, behavior_batch['label'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_step += 1\n",
    "            experiment.log_metric(\"batch_loss\", loss.cpu().detach(), step=training_step)\n",
    "\n",
    "\n",
    "        # 2 - Sample exploratory target commands\n",
    "\n",
    "        top_episodes = replay_buffer.top_episodes(LAST_FEW) # [(S,A,R,S_), ... ]\n",
    "        tgt_horizon = int(np.mean([x[0].shape[0] for x in top_episodes]))\n",
    "        tgt_reward_mean = np.mean([np.sum(x[2]) for x in top_episodes])\n",
    "        tgt_reward_std = np.std([np.sum(x[2]) for x in top_episodes])\n",
    "\n",
    "        def generate_command(tgt_horizon, \n",
    "                            tgt_reward_mean, \n",
    "                            tgt_reward_std):\n",
    "            tgt_horizon = min(tgt_horizon, 200)\n",
    "            tgt_reward = round(np.random.random_sample()*tgt_reward_std + tgt_reward_mean, 0)\n",
    "            return tgt_horizon, tgt_reward\n",
    "\n",
    "        experiment.log_metric(\"tgt_reward_mean\", tgt_reward_mean, step=playing_step)\n",
    "        \n",
    "        \n",
    "        # 3 - Generate new trajectories using latest policy network and generated commands\n",
    "        \n",
    "        for _ in range(NUM_EPISODES_PER_ITER):\n",
    "            episode = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'rewards': [],\n",
    "                'next_states': []\n",
    "            }\n",
    "            episode_reward = 0\n",
    "            # start interactions\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            command_horizon, command_reward = generate_command(tgt_horizon, \n",
    "                                                  tgt_reward_mean, \n",
    "                                                  tgt_reward_std)\n",
    "            \n",
    "            experiment.log_metric(\"command_horizon\", command_horizon, step=playing_step)\n",
    "            experiment.log_metric(\"command_reward\", command_reward, step=playing_step)\n",
    "            while not done:\n",
    "                episode['states'].append(state)\n",
    "                state_ = augment_state(state, \n",
    "                                    command=(command_horizon, command_reward), \n",
    "                                    command_scale=(HORIZON_SCALE, RETURN_SCALE))\n",
    "                state_ = torch.tensor(state_, dtype=torch.float)\n",
    "                with torch.no_grad():\n",
    "                    action_logprobs = policy(state_)\n",
    "                    action_distribution = Categorical(logits=action_logprobs)\n",
    "                    action = action_distribution.sample().item()\n",
    "                state, reward, done, info = env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode['actions'].append(action)\n",
    "                episode['next_states'].append(state)\n",
    "                command_horizon = max(1, command_horizon-1)\n",
    "                if not done: \n",
    "                    episode['rewards'].append(0) # sparse lunar lander\n",
    "                    command_reward -= 0\n",
    "                else:\n",
    "                    episode['rewards'].append(episode_reward)     # sparse lunar lander \n",
    "                    command_reward -= episode_reward\n",
    "            \n",
    "            playing_step += 1\n",
    "            experiment.log_metric(\"episode_reward\", episode_reward, step=playing_step)\n",
    "            \n",
    "\n",
    "            replay_buffer.add_episode(\n",
    "                np.array(episode['states'], dtype=np.float),\n",
    "                np.array(episode['actions'], dtype=np.int),\n",
    "                np.array(episode['rewards'], dtype=np.float),\n",
    "                np.array(episode['next_states'], dtype=np.float),\n",
    "            )\n",
    "except:\n",
    "    env.close()\n",
    "    experiment.end()\n",
    "    print(\"Terminated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
